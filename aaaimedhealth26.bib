@Proceedings{AIMedHealth-2026,
    booktitle = {Proceedings of The Second AAAI Bridge Program on AI for Medicine and Healthcare},
    name = {AAAI Bridge Program on AI for Medicine and Healthcare},
    shortname = {AIMedHealth},
    year = {2026},
    editor = {Wu, Junde and Pan, Jiazhen and Zhu, Jiayuan and Luo, Luyang and Li, Yitong and Xu, Min and Jin, Yueming and Rueckert, Daniel},
    volume = {317},
    start = {2026-1-20},
    end = {2026-1-21},
    address = {Singapore Expo, Singapore},
    conference_url = {https://sites.google.com/view/aimedhealth-aaai/home}, 
}

@InProceedings{ahmed26,
    title = {How Good LLMs Are at Answering Bangla Medical Visual Questions? Dataset and Benchmarking},
    author = {Ahmed, Rafid and Tahmid, Intesar and Hossain, Mir Sazzat and Tomal, Tasnimul Hossain and Fahim, Md and Bhuiyan, Md Farhad Alam},
    pages = {1-14},
    abstract = {Recent advancements in Large Language Models (LLMs) and Large Vision Language Models (LVLMs) have enabled general-purpose systems to demonstrate promising capabilities in complex reasoning tasks, including those in the medical domain. Medical Visual Question Answering (MedVQA) has particularly benefited from these developments. However, despite Bangla being one of the most widely spoken languages globally, there exists no established MedVQA benchmark for it. To address this gap, we introduce BanglaMedVQA, a dataset comprising clinically validated image–question–answer pairs, along with a comprehensive evaluation of current foundation models on this resource. Consistent with prior findings that report low performance of current models on English MedVQA benchmarks, our analysis reveals that Bangla performance is substantially lower, reflecting the challenges inherent to low-resource languages. Even top-performing models such as Gemini and GPT-4.1 mini fail to accurately answer specialized diagnostic questions, indicating severe limitations in fine-grained medical reasoning. Although certain open-source models, such as Gemma-3, occasionally outperform these models in general categories, they too struggle with clinically complex ques- tions, underscoring the urgent need for top-notch evaluation method.}
}

@InProceedings{ajayi26,
    title = {A Machine Learning Approach for Detection of Mental Health Conditions and Cyberbullying from Social Media},
    author = {Ajayi, Edward and Kachweka, Martha and Deku, Mawuli and Aiken, Emily},
    pages = {15-26},
    abstract = {Mental health challenges and cyberbullying are increasingly prevalent in digital spaces, necessitating scalable and interpretable detection systems. This paper introduces a unified multiclass classification framework for detecting ten distinct mental health and cyberbullying categories from social media data. We curate datasets from Twitter and Reddit, implementing a rigorous ’split-then-balance’ pipeline to train on balanced data while evaluating on a realistic, held-out imbalanced test set. We conduct a comprehensive evaluation comparing traditional lexical models, hybrid approaches, and several end-to-end fine-tuned transformers. Our results demonstrate that end-to-end fine-tuning is critical for performance, with the domain-adapted MentalBERT emerging as the top model, achieving an accuracy of 0.92 and a Macro F1 score of 0.76, surpassing both its generic counterpart and a zero-shot LLM baseline. Grounded in a comprehensive ethical analysis, we frame the system as a human-in-the-loop screening aid, not a diagnostic tool. To support this, we introduce a hybrid SHAP-LLM explainability framework and present a prototype dashboard (”Social Media Screener”) designed to integrate model predictions and their explanations into a practical workflow for moderators. Our work provides a robust baseline, highlighting future needs for multi-label, clinically-validated datasets at the critical intersection of online safety and computational mental health.}
}

@InProceedings{akhter26,
    title = {Hide Identity, Preserve Pathology: Diffusion-Based Anonymization for Chest X-rays},
    author = {Akhter, Yasmeena and Dosi, Muskan and Vatsa, Mayank and Singh, Richa},
    pages = {27-36},
    abstract = {Chest X-rays are a widely-used, cost-effective imaging modality for medical investigations; however, they encode distinctive biometric signatures that enable identification attacks. We introduce PrivDiff-Net, a novel diffusion-based framework that addresses critical privacy vulnerabilities in chest X-rays through anatomical biometric features while preserving essential diagnostic utility for clinical applications. Our approach introduces two modules in a latent diffusion framework: (1) a Selective Attribute Suppression (SAS) module that removes sensitive identity cues using orthogonal projection in cross-attention, and (2) a Selective Privacy Guidance (SPG) loss that discourages identity features while preserving diagnostic information during diffusion. Quantitative results show that PrivDiff-Net achieves near-random identification (AUC: 47%) while maintaining high diagnostic accuracy (AUC: 78%). It effectively suppresses sensitive attributes and produces high-quality anonymized CXRs, validated by clinicians for diagnostic utility. These results establish PrivDiff-Net as a new benchmark for privacy-preserving Chest X-rays, providing a practical solution for secure data sharing in collaborative research environments while enabling ethical deployment of AI systems in healthcare where transparency and patient privacy are critical.}
}

@InProceedings{aman26,
    title = {A Systematic Comparison of Data Representations for Transformer-Based ECG Arrhythmia Classification},
    author = {Aman, Mona and Uiso, Godbright and Mukamakuza, Carine and Bhagavatula, Vijayakumar},
    pages = {37-45},
    abstract = {Automated electrocardiogram (ECG) classification plays a key role in detecting cardiac arrhythmias efficiently and objectively. Despite major advances in deep learning, there remains no consensus on whether one-dimensional (1D) temporal or two-dimensional (2D) time–frequency representations yield superior diagnostic accuracy. This study presents a controlled comparison between Vision Transformer (ViT) architectures trained on raw 1D ECG sequences and Short-Time Fourier Transform (STFT)-based 2D spectrograms using the CPSC2018 dataset. Both models share comparable architectures and parameter counts to isolate the effect of signal representation. The 1D-ViT achieved the highest overall accuracy (96.5%) and F1-score (96.5%), confirming that preserving temporal continuity is critical for arrhythmia discrimination. The 2D-ViT achieved lower accuracy (92.6%) due to temporal information loss, though it maintained competitive calibration (AUC 98.6%) and generalization. A bidirectional fusion model combining both encoders through cross-attention exhibited complementary behavior but did not surpass the 1D baseline. These findings indicate that while spectro-temporal information can enhance interpretability and stability, temporal-domain fidelity remains the dominant factor for reliable ECG classification.}
}

@InProceedings{anand26,
    title = {UltrasODM: A Dual Stream Optical Flow Mamba Network for 3D Freehand Ultrasound Reconstruction},
    author = {Anand, Mayank and Alam, Ujair and Prakash, Surya and Shukla, Priya and Nandi, Gora Chand and Puig, Domenec},
    pages = {46-53},
    abstract = {Clinical ultrasound acquisition is highly operator-dependent, where rapid probe motion and brightness fluctuations often lead to reconstruction errors that reduce trust and clinical utility. We present UltrasODM, a dual-stream framework that assists sonographers during acquisition through calibrated per-frame uncertainty, saliency-based diagnostics, and actionable prompts. UltrasODM integrates (i) a contrastive ranking module that groups frames by motion similarity, (ii) an optical-flow stream fused with Dual-Mamba temporal modules for robust 6-DoF pose estimation, and (iii) a Human-in-the-Loop (HITL) layer combining Bayesian uncertainty, clinician-calibrated thresholds, and saliency maps highlighting regions of low confidence. When uncertainty exceeds the threshold, the system issues unobtrusive alerts suggesting corrective actions such as re-scanning highlighted regions or slowing the sweep. Evaluated on a clinical freehand ultrasound dataset, UltrasODM reduces drift by 15.2%, distance error by 12.1%, and Hausdorff distance by 10.1% relative to UltrasOM, while producing per-frame uncertainty and saliency outputs. By emphasizing transparency and clinician feedback, UltrasODM improves reconstruction reliability and supports safer, more trustworthy clinical workflows. Our code is publicly available at https://github.com/AnandMayank/UltrasODM.}
}

@InProceedings{bala26,
    title = {Weight Entropy-Maximised Evidential Metamodel for Post Hoc Uncertainty},
    author = {Bala, Gouranga and Ganatra, Dhruvi and Sethi, Amit},
    pages = {54-59},
    abstract = {Reliable uncertainty quantification (UQ) is crucial for deploying deep learning models in safety-critical domains such as medical imaging. Existing post hoc UQ methods either rely on multi-pass inference or suffer from limited expressiveness due to their dependence on final-layer embeddings. In this work, we propose evidential meta model, a lightweight post-hoc framework that enhances Dirichlet evidential modeling by extracting features from multiple layers of a frozen classifier. This multilayer strategy enriches the metamodel input with both low-level textures and high-level semantics, enabling more accurate modeling of aleatoric and epistemic uncertainty. To further boost epistemic fidelity, we incorporate Max-WEnt regularization, which maximizes the entropy of learnable scaling weights applied within the meta-model. This promotes internal hypothesis diversity without modifying the base network or incurring test-time overhead. Across seven benchmarks including medical datasets (BACH, DIV2K, HAM10000, BreakHIS) and natural image tasks (SVHN, Fashion-MNIST, ImageNet-C) our evidential metamodel consistently improves AUROC and calibration over both the base model and prior post-hoc UQ methods. Ablation studies confirm the complementary benefits of multilayer features and Max-WEnt. Our approach offers a robust and efficient solution for trustworthy AI in clinical and other high stakes settings.}
}

@InProceedings{baruah26,
    title = {Translating Classifier Scores into Clinical Impact: Calibrated Risk and Queueing Simulation for AI-Assisted Radiology Worklist Triage},
    author = {Baruah, Tirthajit and Rathore, Punit},
    pages = {60-66},
    abstract = {Radiology worklists are typically processed on a first-in, first-out (FIFO) basis, even when studies differ greatly in clinical urgency. We propose a pragmatic alternative: using calibrated probabilities of intracranial hemorrhage (ICH) to prioritize head CT exams for earlier reading. Using the public RSNA-ICH dataset, we train slice-level detectors, aggregate them to the exam level, apply post-hoc calibration, and feed these scores into a transparent discrete-event simulator of the reading queue. The simulator quantifies how triage benefits reduction in median time-to-read (TTR) for ICH, which scales with classifier AUC, workload (arrival rate), staffing, prevalence, and calibration. Across realistic loads, score-based prioritization yields substantial TTR reductions for ICH with minimal delay to non-ICH studies. We release a configuration-driven, reproducible pipeline that translates AI risk scores into operational metrics (minutes saved), enabling safe and data-driven evaluation before PACS/RIS1 deployment.}
}

@InProceedings{buol26,
    title = {Architecture-Aware Explainability in ECG Analysis: A Case Study of Aortic Stenosis Detection with ResNet18, LSTM and ViT-MAE ECG},
    author = {Buol, Samuel Chol and Zannu, Julius and Mukamakuza, Carine Pierrette and Olatunji, Damilare Emmanuel and Bhagavatula, Vijayakumar},
    pages = {67-75},
    abstract = {Aortic stenosis (AS) remains a major cardiovascular challenge, as early diagnostic markers in electrocardiogram (ECG) signals are often subtle and difficult to identify using conventional approaches. Although deep learning models have demonstrated strong performance in AS detection, their clinical adoption is limited by the insufficient interpretability of model decisions. Existing explainability studies typically focus on individual architectures, leaving open the question of whether different model designs rely on distinct ECG features. In this work, we investigate how the architecture of neural networks influences the explainability and clinical interpretability in the classification of AS based on ECG. We systematically compare three architectures, namely ResNet18, Long Short-Term Memory (LSTM), and a Vision Transformer with Masked Autoencoder (ViT-MAE),trained in the open-access Cardio-mechanical Signals database comprising 100 patients with valvular heart diseases. All models achieved strong predictive performance, with accuracies of 97.23% (ResNet18), 98.96% (LSTM), and 88.56% (ViT-MAE). To analyze model behavior, we apply both Integrated Gradients and Local Interpretable Model-agnostic Explanations (LIME). The results reveal architecture-specific attribution patterns: ResNet18 exhibits a broad attention across P-waves, QRS complexes, and ST–T segments; LSTM emphasizes temporally salient QRS related features; and ViT-MAE prioritizes repolarization associated regions, including T-waves and QT intervals. Despite these differences, all architectures consistently focus on clinically meaningful ECG regions associated with AS pathophysiology. These findings demonstrate that explainability outcomes are strongly influenced by model architecture and underscore the importance of architecture-aware interpretability strategies for building transparent, reliable and clinically trustworthy AI systems for cardiovascular diagnosis.}
}

@InProceedings{chung26,
    title = {Modality-Specific Enhancement and Complementary Fusion for Semi-Supervised Multi-Modal Brain Tumor Segmentation},
    author = {Chung, Tien-Dat and Lam, Ba-Thinh and Nguyen, Thanh-Huy and Nguyen, Thien and Vu, Nguyen Lan Vi and Cao, Hoang-Loc and Huynh, Phat K. and Xu, Min},
    pages = {76-84},
    abstract = {Semi-supervised learning (SSL) has become a promising direction for medical image segmentation, enabling models to learn from limited labeled data alongside abundant unlabeled samples. However, existing SSL approaches for multi-modal medical imaging often struggle to exploit the complementary information between modalities due to semantic discrepancies and misalignment across MRI sequences. To address this, we propose a novel semi-supervised multi-modal framework that explicitly enhances modality-specific representations and facilitates adaptive cross-modal information fusion. Specifically, we introduce a Modality-specific Enhancing Module (MEM) to strengthen semantic cues unique to each modality via channel-wise attention, and a learnable Complementary Information Fusion (CIF) module to adaptively exchange complementary knowledge between modalities. The overall framework is optimized using a hybrid objective combining supervised segmentation loss and cross-modal consistency regularization on unlabeled data. Extensive experiments on the BraTS 2019 (HGG subset) demonstrate that our method consistently outperforms strong semi-supervised and multi-modal baselines under 1%, 5%, and 10% labeled data settings, achieving significant improvements in both Dice and Sensitivity scores. Ablation studies further confirm the complementary effects of our proposed MEM and CIF in bridging cross-modality discrepancies and improving segmentation robustness under scarce supervision.}
}

@InProceedings{dacon26a,
    title = {Optimizing Insulin Dosing for Type 1 Diabetes with Thyroid Dysfunction Using Q-Learning: A Personalized Approach to Chronic Disease Management},
    author = {Dacon, Jamell and Uwaeme, Chukwulenyeudo and Obasi, Chukwuemeka and Soji-John, Oluwasegun and Olajide, Oluwatobi and Savage, Marissa and Ayodele, Iyinoluwa and King, Oluwajomiloju and Minard, Chelsea and Mosuro, Michael and Wojuade, Obaloluwa and Somerville, Nicholaus and Brown, Mikayla and Hallowell, Abdulai Thomas and Nunnally, Nyah},
    pages = {85-93},
    abstract = {Thyroid dysfunction frequently coexists with Type 1 Diabetes (T1D), creating complex clinical challenges due to the critical interplay between thyroid hormone fluctuations and insulin sensitivity. Existing insulin dosing protocols typically do not account for these dynamic comorbid interactions, often leading to suboptimal glycemic control and increased adverse event risk. To address this gap and prioritize the clinical interpretability necessary for adoption, we propose a novel Reinforcement Learning (RL) framework based on tabular Q-learning that explicitly models discrete thyroid dysfunction severity within the patient state and incorporates the delayed pharmacodynamic effects of thyroid medications into a dual-objective reward function. This deliberate design enables personalized, transparent insulin dosing policies that optimize both glycemic control and thyroid hormone stabilization. We evaluate our approach on the real-world T1DGranada cohort comprising adults with T1D and hypothyroidism. Our comorbidity-aware, interpretable model achieves a 15% improvement in Time-in-Range (TIR) and a 42% reduction in hypoglycemic events compared to standard clinical baselines, while also significantly enhancing thyroid hormone stabilization rates. Offline evaluation techniques including importance sampling and Fitted Q-Evaluation (FQE) validate the robustness and reliability of the learned policies. Furthermore, expert endocrinologist blind review confirms high clinical alignment with 83% agreement. These results underscore the importance of explicitly modeling multimorbidity and delayed treatment effects in interpretable RL frameworks to advance personalized chronic disease management and facilitate clinical trust and integration.}
}

@InProceedings{dacon26b,
    title = {Manifold-Informed Cohort Discovery (MICD): A Framework for Uncovering Latent Risk Signals in Imbalanced Healthcare Data},
    author = {Dacon, Jamell and Minard, Chelsea and Olajide, Oluwatobi and Uwaeme, Chukwulenyeudo and Obasi, Chukwuemeka and Mosuro, Michael and Soji-John, Oluwasegun and Ayodele, Iyinoluwa},
    pages = {94-101},
    abstract = {Risk stratification for Coronary Heart Disease (CHD) is fundamentally challenged by severe class imbalance and the structural heterogeneity of the non-diseased patient cohort. Standard classification models, by treating all CHD-negative patients uniformly, fail to detect critical, latent high-risk sub-groups. We introduce the Manifold-Informed Cohort Discovery (MICD) Framework, a novel methodology that systematically integrates clinically-informed feature selection, Manifold Learning (UMAP), and proximity-based clustering to extract these latent risk signals. Our core insight is that individuals with latent high-risk profiles exist in close geometric proximity to true CHD-positive cases within the UMAP-embedded feature space. We validate the framework’s clinical relevance by autonomously isolating a high-risk negative cohort whose feature profile strongly aligns with the established diagnostic markers of Metabolic Syndrome. This alignment proves that our abstract geometric approach encodes a biologically and clinically meaningful pre-disease state. When the insights from this cohort discovery are used in a downstream classification task, the MICD-enhanced model achieves pre-eminent predictive performance (AUROC ∼ 85.1%), significantly outperforming the clinical gold standard (ASCVD Risk Calculator) and state-of-the-art imbalanced learning methods (Focal Loss, SMOTE). Our work establishes a critical, interpretable link between unsupervised data structure and actionable supervised clinical prediction, providing a powerful tool for early, preventative intervention.}
}

@InProceedings{ghosh26,
    title = {Integrating AI-Driven Triage into Digital Pharmacy Systems for Rational Antibiotic Use in Low-Resource Settings},
    author = {Ghosh, Emon and Rahman, Md. Jobayer and Ahamed, Shamim and Salwa, Marium and Foyez, Tahmina and Mamun, Khondaker A.},
    pages = {102-109},
    abstract = {Antimicrobial resistance (AMR) is becoming a bigger threat to global health, especially in low- and middle-income countries (LMICs), where antibiotics are often given out without a prescription. This study presents a hybrid Artificial Intelligence (AI) framework to improve rational antibiotic utilization by integrating pharmacies, telemedicine practitioners, and policymakers through an integrated digital infrastructure. The digital intervention system was implemented in 28 community pharmacies and showed significant improvements in how people used antibiotics. Specifically, the percentage of people who self-medicated with antibiotics dropped from 4.05% to 0.86%, and the percentage of people who bought antibiotics with a prescription jumped from 27.6% to 43.5%. We propose an AI framework that combines a machine learning (ML)-based and a large language model (LLM)-based symptom checker for intelligent triage and clinical decision support. These models will enable efficient analysis of both structured and narrative symptom data, which will help pharmacists provide advice and doctors refer patients right away. The suggested model demonstrates a scalable, data-driven, and human-in-the-loop approach to antibiotic stewardship, with the potential to support future AMR mitigation and healthcare accessibility in LMICs settings.}
}

@InProceedings{gorade26,
    title = {L2GNet: Optimal Local-to-Global Representation of Anatomical Structures for Generalized Medical Image Segmentation},
    author = {Gorade, Vandan and Singhal, Rekha and Dasu, Neethi and Mittal, Sparsh and Santosh, KC and Jha, Debesh},
    pages = {110-116},
    abstract = {Continuous Latent Space (CLS) and Discrete Latent Space (DLS) models, like AttnUNet and VQUNet, have excelled in medical image segmentation. In contrast, Synergistic Continuous and Discrete Latent Space (CDLS) models show promise in handling fine and coarse-grained information. However, they struggle with modeling long-range dependencies. CLS or CDLS-based models, such as TransUNet or SynergyNet are adept at capturing long-range dependencies. Since they rely heavily on feature pooling or aggregation using self-attention, they may capture dependencies among redundant regions. This hinders comprehension of anatomical structure content, poses challenges in modeling intra-class and inter-class dependencies, increases false negatives and compromises generalization. Addressing these issues, we propose L2GNet, which learns global dependencies by relating discrete codes obtained from DLS using optimal transport and aligning codes on a trainable reference. L2GNet achieves discriminative on-the-fly representation learning without an additional weight matrix in self-attention models, making it computationally efficient for medical applications. Extensive experiments on multiorgan segmentation and cardiac datasets demonstrate L2GNet’s superiority over state-of-the-art methods, including the CDLS method SynergyNet, offering a novel approach to enhance deep learning models’ performance in medical image analysis.}
}

@InProceedings{gu26,
    title = {Clinical-R1: Empowering Large Language Models for Faithful and Comprehensive Reasoning with Clinical Objective Relative Policy Optimization},
    author = {Gu, Boyang and Zhou, Hongjian and Segal, Bradley Max and Wu, Jinge and Cao, Zeyu and Zhong, Hantao and Clifton, Lei and Liu, Fenglin and Clifton, David A.},
    pages = {117-126},
    abstract = {Recent advances in large language models (LLMs) have shown strong reasoning capabilities through large-scale pre-training and post-training reinforcement learning, demonstrated by DeepSeek-R1. However, current post-training methods, such as Grouped Relative Policy Optimization (GRPO), mainly reward correctness, which is not aligned with the multi-dimensional objectives required in high-stakes fields such as medicine, where reasoning must also be faithful and comprehensive. We introduce Clinical-Objective Relative Policy Optimization (CRPO), a scalable, multi-objective, verifiable reinforcement learning method designed to align LLM post-training with clinical reasoning principles. CRPO integrates rule-based and verifiable reward signals that jointly optimize accuracy, faithfulness, and comprehensiveness without relying on human annotation. To demonstrate its effectiveness, we train Clinical-R1-3B, a 3B-parameter model for clinical reasoning. The experiments on three benchmarks demonstrate that our CRPO substantially improves reasoning on truthfulness and completeness over standard GRPO while maintaining comfortable accuracy enhancements. This framework provides a scalable pathway to align LLM reasoning with clinical objectives, enabling safer and more collaborative AI systems for healthcare while also highlighting the potential of multi-objective, verifiable RL methods in post-training scaling of LLMs for medical domains. Our data, models, and code are all publicly available on https://github.com/BoyangGu1/Clinical-R1-3B.}
}

@InProceedings{gupta26a,
    title = {FedHypeVAE: Federated Learning with Hypernetwork-Generated Conditional VAEs for Differentially-Private Embedding Sharing},
    author = {Gupta, Sunny and Jangid, Nikita and Sethi, Amit},
    pages = {127-134},
    abstract = {Federated learning enables collaborative model development across medical institutions without centralizing sensitive patient data, yet existing embedding-level generative approaches often degrade under non-IID clinical heterogeneity and offer limited formal protection against gradient leakage. We introduce FedHypeVAE, a differentially private, hyper-network based conditional variational framework that generates client-specific decoders and priors from lightweight, trainable client codes. This bi-level formulation personalizes the generative process while ensuring privacy preserving parameter synthesis decoupled from raw medical images. Federated optimization with differential privacy and distributional alignment strategies improves stability and cross-site generalization. The proposed framework unifies personalization, privacy, and domain adaptability within the generative layer, offering a principled solution for privacy-aware representation learning in multi institutional medical imaging. Code: github.com/sunnyinAI/FedHypeVAE}
}

@InProceedings{gupta26b,
    title = {FedNeuro: Multi-Site fMRI Analysis Using Hypernetwork Personalized and Privacy Enhanced Federated Learning},
    author = {Gupta, Sunny and Shanker, Shambhavi and Sethi, Amit},
    pages = {135-140},
    abstract = {Multi-site functional MRI (fMRI) studies enable comprehensive understanding of brain disorders by integrating data across institutions, yet centralized model training remains limited by privacy regulations and domain heterogeneity. We propose FedNeuro, a hypernetwork-personalized and privacy-enhanced federated learning framework for collaborative fMRI analysis. Unlike conventional federated averaging, FedNeuro employs a global hypernetwork that generates site-specific model parameters from private client embeddings, allowing each site to learn personalized representations while maintaining global consistency. This bi-level meta-optimization decouples data-dependent gradients from shared parameters, providing structural privacy protection and improving cross-site generalization. Evaluated on the multi-site ABIDE dataset, FedNeuro achieves robust gains over federated baselines, marking a step toward scalable, privacy-preserving, and domain-fair neuroimaging for precision neuroscience. Code: https://github.com/sunnyinAI/FedNeuro}
}

@InProceedings{issah26,
    title = {Bridging the Gap in Malaria Diagnostics: An Attention-Centric YOLO Framework with Species-Specific Augmentation for Tiny Parasite Detection in Low-Resource Settings},
    author = {Issah, Ahmed Tahiru and Mukamakuz, Carine},
    pages = {141-149},
    abstract = {Malaria remains a major health challenge in Africa, with accurate species identification critical for prompt treatment and control, especially in resource-limited settings like Rwanda. This study systematically benchmarks state-of-the-art detection architectures for automated multi-species malaria parasite detection from high-resolution Giemsa-stained blood smear images, addressing the persistent problems of class imbalance and the difficulty of detecting small Plasmodium falciparum ring forms. We evaluate and optimize YOLO-SPAM, YOLO-Para, and YOLOv12 models, applying a novel species-specific augmentation protocol with copy-paste and noise injection. YOLOv12, trained with these protocols, achieves outstanding overall performance (0.878 mAP50, 0.677 mAP50−95) and demonstrates significant improvement in detecting small, clinically relevant P. falciparum parasites over non-attention YOLO methods). Comparative experiments reveal that targeted data augmentation and strategic model selection can overcome significant class imbalance, achieving reliable multi-species differentiation and precise localization even for morphologically subtle classes. Our findings validate the practical promise of advanced object detection models, such as YOLOv12, for malaria diagnosis workflows and support their future deployment in real-world microscopy labs in Rwanda and similar endemic regions.}
}

@InProceedings{jia26,
    title = {Segmentation-Guided Radiology Report Generation for Pneumothorax Detection in Chest X-Rays},
    author = {Jia, Yiming and Elboardy, Ahmed T. and Rashed, Essam A.},
    pages = {150-158},
    abstract = {Recent developments on chest radiographs has primarily focused on developing multi-disease frameworks that aim to diagnose a wide range of thoracic abnormalities from the Chest X-ray datasets. In contrast, this study specifically targets pneumothorax, a life-threatening condition commonly referred to as a collapsed lung, which requires timely detection and accurate clinical reporting. Existing automated report generation Vision-Language Models (VLMs) mainly rely on image-level features and often fail to fully leverage the rich structural information embedded in medical image segmentation. To address this limitation, we propose a distinct strategy to incorporate pneumothorax segmentation masks, which delineate affected regions and provide precise localization guidance to enhance the accuracy of medical image interpretation. Experimental results demonstrate that the proposed segmentation-guided approach integrates visual and textual understanding more effectively for pneumothorax diagnosis from chest radiographs. By employing segmentation masks as guidance, VLMs can accurately localize pathological regions while preserving anatomical context, thereby improving both interpretability and diagnostic precision. Quantitative evaluations across multiple metrics further confirm the effectiveness of the proposed methods in bridging the gap between image-level localization and report-level reasoning.}
}

@InProceedings{karagoz26,
    title = {XIME3D: A Systematic Framework for Evaluating Explainable AI in 3D Medical Imaging under CT Image Pre-Processing Variations},
    author = {Karagoz, Gizem and Ozcelebi, Tanir and Meratnia, Nirvana},
    pages = {159-168},
    abstract = {Recent advancements in deep learning have enabled expert-level performance in medical imaging for disease classification, but their black-box decision making processes limit trust in them and their wide-spread clinical deployment. While Explainable Artificial Intelligence (XAI) methods aim to bridge this gap, studies focus on 2D data or pre-processed research datasets that overlook the role of medical imaging pre-processing operations which is an essential component of real-world 3D medical imaging workflows. To address this limitation, we propose XIME3D, a systematic and predictive model–centered framework for evaluating explainability under realistic medical pre-processing conditions for volumetric medical data. The framework integrates five volumetric pre-processing variants and ten post-hoc attribution methods, evaluated through three complementary criteria: Correctness, Contrastivity, and Completeness, which together evaluate explanation dependence on model input, internal structure, and output behavior. Across more than 300 experimental configurations, XIME3D reveals that gradient-based methods, such as Integrated Gradients and Blur Integrated Gradients, provide the most consistent and model-aligned explanations, while noise-based approaches like SmoothGrad and VarGrad are less sensitive to model behavior. These findings emphasize the importance of clinically realistic evaluation pipelines for reliable explainability in 3D medical imaging.}
}

@InProceedings{kulkarni26,
    title = {All Required, In Order: Phase-Level Evaluation for AI–Human Dialogue in Healthcare and Beyond},
    author = {Kulkarni, Shubham and Lyzhov, Alexander and Chaitanya, Shiva and Joshi, Preetam},
    pages = {169-180},
    abstract = {Conversational AI is starting to support real clinical work, but most evaluation methods miss how compliance depends on the full course of a conversation. We introduce Obligatory-Information Phase Structured Compliance Evaluation (OIP–SCE), an evaluation method that checks whether every required clinical obligation is met, in the right order, with clear evidence for clinicians to review. This makes complex rules practical and auditable, helping close the gap between technical progress and what healthcare actually needs. We demonstrate the method in two case studies (respiratory history, benefits verification) and show how phase-level evidence turns policy into shared, actionable steps. By giving clinicians control over what to check and engineers a clear specification to implement, OIP–SCE provides a single, auditable evaluation surface that aligns AI capability with clinical workflow and supports routine, safe use.}
}

@InProceedings{narendra26,
    title = {Towards Reliable Few-Shot Adaptation of Pathology Foundation Models via Conformal Prediction},
    author = {Narendra, Aditya and Panda, Subhankar and Maurya, Chandresh Kumar},
    pages = {181-190},
    abstract = {Recent advances in foundation models have enabled their integration into high-stakes clinical settings, particularly in computational pathology, where domain-specialized FMs demonstrate strong generalization. However, real-world deployment is constrained by their poorly calibrated uncertainty awareness and degraded performance in low-data regimes requiring few-shot adaptation strategies, leading to unreliable and inefficient diagnostic workflows. Conformal Prediction (CP) is an uncertainty quantification framework that offers distribution-free, finite-sample coverage guarantees for ensuring safer deployment in such settings. In this work, we explore the integration of various CP methods with pathology foundation models using three few-shot adaption strategies for classification tasks across two datasets. To assess the clinical effectiveness of these approaches, we propose four novel metrics aimed at improving clinical reliability and alleviating diagnostic workload in few-shot settings. Our results demonstrate that Conformal Prediction methods enhance the reliability of pathology foundation models and offer actionable uncertainty estimates to enable safe and efficient deployment in few-shot pathological classification workflows, with the LAC method achieving the best overall performance. Code is available at https://github.com/AdiNarendra98/Few-Shot-PathCP.}
}

@InProceedings{pokharel26,
    title = {From Policy to Logic for Efficient and Interpretable Coverage Assessment},
    author = {Pokharel, Rhitabrat and Hassanzadeh, Hamid Reza and Agrawal, Ameeta},
    pages = {191-200},
    abstract = {Large Language Models (LLMs) have demonstrated strong capabilities in interpreting lengthy, complex legal and policy language. However, their reliability can be undermined by hallucinations and inconsistencies, particularly when analyzing subjective and nuanced documents. These challenges are especially critical in medical coverage policy review, where human experts must be able to rely on accurate information. In this paper, we present an approach designed to support human reviewers by making policy interpretation more efficient and interpretable. We introduce a methodology that pairs a coverage-aware retriever with symbolic rule-based reasoning to surface relevant policy language, organize it into explicit facts and rules, and generate auditable rationales. This hybrid system minimizes the number of LLM inferences required which reduces overall model cost. Notably, our approach achieves a 44% reduction in inference cost alongside a 4.5% improvement in F1 score, demonstrating both efficiency and effectiveness.}
}

@InProceedings{roy26,
    title = {AI-Instigated Human Oversight: Rethinking Human-in-the-Loop Safety in Clinical AI},
    author = {Roy, Sera Singha},
    pages = {201-209},
    abstract = {The rise in clinical AI has helped to define the diagnostic and decision-making process in the dimension of modern medicine. However, their implementation is constrained by multiple ethical, interpretability, and safety factors that hinder the effective utilization of these AI systems. Existing Human-in-the-Loop (HITL) systems rely heavily on externally triggered human oversight, which creates critical gaps in effective safety and accountability. This study introduces the AI-Instigated Human Oversight (AIHO) framework, an AI governance architecture that enables models to self-assess uncertainty or contextual failure and autonomously escalate decisions to human oversight through a four-layer detection and escalation mechanism. Each layer performs a distinct self-assessment: (Layer 1) predictive uncertainty quantification, (Layer 2) contextual validation and explainability, (Layer 3) ethical and proxy alignment monitoring, and (Layer 4) adaptive governance with human-in-command enforcement. These layers form part of a three-zone operational architecture: (1) Information Flow and AI Core Processing, (2) AIHO Oversight Core, and (3) Human Escalation Loops. These zones in conjunction create a continuous feedback loop which administers continuous self-evaluation, transforming ethical and technical anomalies into actionable human oversight triggers. AIHO establishes a dynamic pathway toward regulation-ready, self-aware clinical AI, aligning with the current international standards for trustworthy and accountable AI in medicine. This paper presents the conceptual architecture and mathematical formalization of AIHO; empirical validation across clinical domains represents the critical next phase of this research.}
}

@InProceedings{sagar26a,
    title = {LightFusionNet: Lightweight Dual-Stream Network with Predictive Context Attention for Efficient Medical Image Fusion},
    author = {Sagar, Abhinav},
    pages = {210-218},
    abstract = {Multimodal image fusion aims to integrate complementary information from multiple imaging modalities into a single, informative representation, which is crucial for applications in medical imaging and microscopy. Existing methods often face trade-offs between structural fidelity, edge preservation, and computational efficiency. In this work, we propose Light-FusionNet, a lightweight dual-stream network designed to efficiently fuse multimodal images while retaining key structural, textural, and intensity features. The network leverages depthwise separable convolutions to reduce model complexity and incorporates a Predictive Context Attention (PCA) mechanism to selectively emphasize informative regions in the feature maps. Extensive experiments on benchmark medical imaging datasets, including PET-MRI, SPECT-MRI, and CT-MRI, demonstrate that our approach achieves comparable qualitative and quantitative performance compared to state-of-the-art fusion methods, while maintaining low computational cost. The proposed method provides an effective and efficient solution for multimodal image fusion, suitable for both clinical and research applications.}
}

@InProceedings{sagar26b,
    title = {BUCAN: Bayesian Uncertainty-aware Classification with Attention Networks for Medical Images},
    author = {Sagar, Abhinav},
    pages = {219-228},
    abstract = {Accurate and reliable medical image classification is critical for clinical decision-making across diverse imaging modalities, including X-ray, CT, and MRI. Traditional convolutional neural networks often produce overconfident predictions, limiting their clinical trustworthiness. In this work, we propose an uncertainty-aware, attention-augmented neural network that integrates multi-scale SwirlAttention and FeedBackAttention modules with a Bayesian probabilistic classifier. This framework enables robust feature extraction, interpretable attention maps, and principled estimation of epistemic uncertainty. We evaluate our approach on four diverse datasets, including Diabetic Retinopathy, Kvasir, Skin Cancer, and fused multi-focal Oocyte images, covering a wide range of pathological and morphological variations. Extensive experiments demonstrate that our method outperforms state-of-the-art CNN and transformer-based baselines in terms of accuracy, calibration, and interpretability. Grad-CAM visualizations highlight clinically relevant regions, while uncertainty estimates provide actionable insights for ambiguous cases, making the framework suitable for reliable deployment in real-world clinical settings.}
}

@InProceedings{sagar26c,
    title = {TAFIE: Transformer-Assisted Fusion with Integrated Entropy Attention for Multimodal Medical Imaging},
    author = {Sagar, Abhinav},
    pages = {229-238},
    abstract = {Multimodal medical image fusion aims to integrate complementary information from different imaging modalities to enhance clinical diagnosis and surgical navigation. While deep learning-based approaches have significantly advanced fusion quality over traditional methods by leveraging powerful feature extraction, challenges such as blurring, noise, and artifacts persist in the fused results. To address these issues, we propose a novel fusion framework that incorporates an entropy-based attention module to emphasize salient image regions. Our architecture is designed in a multi-scale manner, utilizing an adaptive gating mechanism to effectively extract and combine salient features across different scales. Additionally, we introduce a Top-K token vision transformer to enable efficient global feature extraction while reducing computational overhead by restricting the context space. We further demonstrate the effectiveness of our fused representations in the downstream task of oocyte quality prediction, showing improved accuracy over individual focal images as well as over other approaches. Extensive experiments on diverse medical imaging datasets demonstrate that our method achieves competitive performance compared to state-of-the-art techniques, both quantitatively and visually. Ablation studies underscore the importance of each proposed component.}
}

@InProceedings{stothers26,
    title = {Radiation-Preserving Selective Imaging for Pediatric Hip Dysplasia: A Cross-Modal Ultrasound-Xray Policy with Limited Labels},
    author = {Stothers, Duncan and Stothers, Ben and Schaeffer, Emily and Mulpuri, Kishore},
    pages = {239-246},
    abstract = {We study an ultrasound-first, radiation-preserving policy for developmental dysplasia of the hip (DDH) that requests an X-ray (XR) only when needed. We (i) pretrain modality-specific encoders (ResNet-18) with SimSiam on a large unlabelled registry (37,186 ultrasound; 19,546 radiographs), (ii) freeze the backbones and fit small, measurement-faithful heads on DDH-relevant landmarks and measurements, (iii) calibrate a one-sided conformal deferral rule on ultrasound predictions that provides finite-sample marginal coverage guarantees under exchangeability, using a held-out calibration set. Ultrasound heads predict Graf α/β and femoral head coverage; X-ray heads predict acetabular index (AI), center-edge (CE) angle and IHDI grade. On our held-out labeled evaluation set, ultrasound measurement error is modest (e.g., α MAE ≈ 9.7◦, coverage MAE ≈ 14.0 percentage points), while radiographic probes achieve AI and CE MAEs of ≈ 7.6◦ and ≈ 8.9◦, respectively. The calibrated US-only policy is explored across rule families (alpha-only; alpha OR coverage; alpha AND coverage), conformal miscoverage levels (δα,δcov), and per-utility trade-offs using decision-curve analysis. Conservative settings yield high coverage (e.g., ∼0.90 for α) with near-zero US-only rates; permissive settings (e.g., alpha OR coverage at larger deltas) achieve non-zero US-only throughput with expected coverage trade-offs. The result is a simple, reproducible pipeline that turns limited labels into interpretable measurements and tunable selective imaging curves suitable for clinical handoff and future external validation.}
}

@InProceedings{tomar26,
    title = {Prototype Learning for Out-of-Distribution Polyp Segmentation},
    author = {Tomar, Nikhil Kumar and Jha, Debesh and Bagci, Ulas},
    pages = {247-254},
    abstract = {Existing polyp segmentation models from colonoscopy images often fail to provide reliable segmentation results on datasets from different centers, limiting their applicability. Our objective in this study is to create a robust and well-generalized segmentation model named PrototypeLab that can assist in polyp segmentation. To achieve this, we incorporate various lighting modes such as White light imaging (WLI), Blue light imaging (BLI), Linked color imaging (LCI), and Flexible spectral imaging color enhancement (FICE) into our new segmentation model, which learns to create prototypes for each class of object present in the images. These prototypes represent the characteristic features of the objects, such as their shape, texture, and color. Our model is designed to perform effectively on out-of-distribution (OOD) datasets from multiple centers. We first generate a coarse mask that is used to learn prototypes for the main object class, which are then employed to generate the final segmentation mask. By using prototypes to represent the main class, our approach handles the variability present in the medical images and generalizes well to new data since prototypes capture the underlying distribution of the data. PrototypeLab offers a promising solution with a dice coefficient of ≥ 90% and mIoU ≥ 85% with a near real-time processing speed for polyp segmentation. It achieved superior performance on OOD datasets compared to 16 state-of-the-art image segmentation architectures, potentially improving clinical outcomes. Codes will be made available at https://github.com/nikhilroxtomar/PrototypeLab.}
}

@InProceedings{ujjain26,
    title = {GlucoGrapher: Sensor-Aware CGM Fusion with Mask-Aware Meta-Ensembles for Predicting Carbohydrate Caloric Ratio from Postprandial Glucose},
    author = {Ujjain, Siddhant and Singh, Pooja and Srivastava, Ekta and Hashmi, Ahmad Siraj and Kumar, Sandeep and Gandhi, Tapan Kumar},
    pages = {255-264},
    abstract = {We study meal-level inference of the carbohydrate calorie ratio (CCR), the fraction of total calories attributable to net carbohydrates directly from early postprandial glucose responses (PPGR) recorded by continuous glucose monitors (CGMs). Using the date-shifted CGMACROS v1.0.0, we introduce GLUCOGRAPHER, a deployment-minded pipeline that (i) aligns PPGR around meal onset, (ii) performs sensor-aware fusion of Dexcom/Libre traces with explicit discordance detection and gating, (iii) leverages a dual preprocessing view of the signal (absolute ∆mg/dL and percent change relative to baseline) with per-subject PPGR standardization to mitigate inter-individual scale shifts, and (iv) applies fold-wise, mask-aware nonnegative meta-learning with isotonic calibration to combine heterogeneous and sometimes missing out-of-fold (OOF) predictions without leakage. We augment PPGR shape descriptors (peak/time-to-peak, IAUC windows, slopes, late-ratio features) with lightweight behavior (steps, heart rate) and compact subject context (BMI, HbA1c buckets, selected fasting labs, and up to eight microbiome principal components). Evaluated with 5-fold Group-KFold by participant over n=663 meals, GLUCOGRAPHER attains RMSE 0.0929, NRMSErange 0.1608, NRMSEstd 0.7229, and Pearson r 0.6910. Performance is consistent across HbA1c-defined strata (Healthy/PreDM/T2D), indicating robustness to baseline glycemic status. Ablations show that the mask-aware meta-ensemble delivers a substantive lift over calibrated tree baselines, highlighting the value of reliability-aware sensor fusion, dual preprocessing, and leak-free calibration. Framing CCR as a bounded, interpretable target in [0, 1] enables actionable CGM-only feedback without perfect food logging, supporting retrospective coaching and prospective planning.}
}

@InProceedings{urooj26,
    title = {XAI-MeD: Explainable Knowledge Guided Neuro-Symbolic Framework for Domain Generalization and Rare Class Detection in Medical Imaging},
    author = {Urooj, Midhat and Banerjee, Ayan and Gupta, Sandeep},
    pages = {265-274},
    abstract = {Explainability, domain generalization, and rare-class reliability are critical challenges in medical AI, where deep models often fail under real-world distribution shifts and exhibit bias against infrequent clinical conditions. This paper introduces XAI-MeD, an explainable medical AI framework that integrates clinically accurate expert knowledge into deep learning through a unified neuro-symbolic architecture. XAI-MeD is designed to improve robustness under distribution shift, enhance rare-class sensitivity, and deliver transparent, clinically aligned interpretations. The framework encodes clinical expertise as logical connectives over atomic medical propositions, transforming them into machine-checkable, classspecific rules. Their diagnostic utility is quantified through weighted feature satisfaction scores, enabling a symbolic reasoning branch that complements neural predictions. A confidence-weighted fusion integrates symbolic and deep outputs, while a Hunt-inspired adaptive routing mechanism—guided by Entropy Imbalance Gain (EIG) and Rare-Class Gini mitigates class imbalance, high intra-class variability, and uncertainty. We evaluate XAI-MeD across diverse modalities, on four challenging tasks: (i) Seizure Onset Zone (SOZ) localization from rs-fMRI, (ii) Diabetic Retinopathy grading, across 6 multicenter datasets demonstrate substantial performance improvements, including 6% gains in cross- domain generalization and a 10% improved rare-class F1 score far outperforming state-of-the-art deep learning baselines. Ablation studies confirm that the clinically grounded symbolic components act as effective regularizers, ensuring robustness to distribution shifts. XAI-MeD thus provides a principled, clinically faithful, and interpretable approach to multimodal medical AI.}
}

@InProceedings{vallabhaneni26,
    title = {VEIL: A Framework for Differentially Private, Interpretable, and Communication-Efficient Federated Learning},
    author = {Vallabhaneni, Sunith},
    pages = {275-283},
    abstract = {Federated Learning (FL) promises to unlock the potential of multi-institutional clinical data by enabling collaborative model training without centralizing sensitive patient information. However, practical adoption has been critically hindered by a trifecta of conflicting challenges: ensuring formal patient privacy, overcoming the ”black box” nature of models which erodes clinical trust, and managing the prohibitive communication costs of standard algorithms. In this work, we introduce VEIL (DP–Verified, Efficient, Interpretable, (Federated) Learning), a novel FL framework designed from the ground up to resolve these trade-offs. VEIL employs a federated concept evolution paradigm where clients privately propose salient clinical features, and a global model is constructed from a validated consensus. Our experiments on a real-world, multi-center ICU mortality prediction task demonstrate that VEIL presents a holistically superior solution. The final, calibrated VEIL model achieves competitive discriminative performance (AUC 0.835), on par with strong non-private baselines, while reducing communication overhead by over 90% and attaining best-in-class trustworthiness (ECE 0.010). We showcase VEIL’s primary contribution—deep, instance-level interpretability—through clinical explanation dashboards that translate predictions into transparent, actionable insights. By holistically addressing the core barriers to adoption, VEIL provides a practical and trustworthy pathway for deploying federated learning in real-world medical settings. To facilitate reproducibility and further research, our implementation and the full set of hyperparameters will be made publicly available upon publication.}
}

@InProceedings{wang26,
    title = {Iterative Refinement of Radiation Therapy Dose Distribution Prediction for Accelerated Partial Breast Irradiation via Plan Scoring},
    author = {Wang, Ledi and McBeth, Rafe},
    pages = {284-292},
    abstract = {The integration of artificial intelligence into clinical workflows offers substantial opportunities to enhance treatment quality in radiation oncology but also require methods that address the complexity of clinical decision making. Recent advances in dose prediction demonstrate the value of conditioning on patient specific anatomy. However, the tendency of deep learning models to regress toward cohort averages motivates strategies that preferentially learn from higher quality exemplars. Building on these insights, we present a framework that uses clinically relevant metrics to refine prediction quality for accelerated partial breast irradiation. A linear piecewise scoring system assigns normalized scores to each plan across dose volume histogram metrics covering target coverage and sparing of organs at risk. We applied this framework to a retrospective cohort of 550 patients treated at our institution, and we trained a three dimensional dose prediction model based on a hierarchically dense U Net. Preliminary results reveal wide variation in score distributions across the cohort, reflecting both patient specific complexity and variation in planning quality. We trained a baseline model and then applied our iterative refinement framework, which ranks cases by the composite quality score and, at each round, retains the highest scoring half for retraining. Models refined in this manner demonstrated promising improvements in predicted plan quality, supporting the effectiveness of this approach. These findings illustrate how scoring guided curation can align model behavior with clinical priorities and provide a path toward adaptive, high precision treatment planning tools.}
}

@InProceedings{wu26,
    title = {Agent-Memory Protocol: A Privacy-Focused Protocol for LLM Agents and User Memory Interaction},
    author = {Wu, Junde and Hu, Minhao and Zhu, Jiayuan and Wang, Jiaye and Jin, Yueming},
    pages = {293-301},
    abstract = {Large-Language Model (LLM) based agents are rapidly migrating from stateless chat systems to persistent, longitudinal assistants. Nowhere is this transition more sensitive than in fields like medicine and finance, where context accumulates across months of interactions containing personally identifiable or confidential information. Existing memory and retrieval architectures of agents focus on efficiency or reasoning quality but remain indifferent to privacy: prompts, retrieval results, and cached traces leak explicit identifiers to third-party infrastructure. We introduce the Agent-Memory Protocol (AMP), a privacy-first protocol for LLM Agents and User Memory Interaction. AMP enforces confidentiality at the boundary where language meets computation. It defines three deterministic operations—redact at rest, pack for purpose, and hydrate on return, that together guarantee that no personal identifier ever leaves the user boundary while maintaining the reasoning utility of long-term memory. We formalize its protocol and illustrate its operation in multi-turn use-cases spanning medicine and finance, including radiology follow-up, discharge planning, and portfolio management.}
}

@InProceedings{xin26,
    title = {DualCPT: Dual-branch Modeling for Cellular Phenotype Transition},
    author = {Xin, Lei and Kong, Zhenglun and Chen, Fukang and Zheng, Yuhao and Wang, Zeheng and Tang, Hao},
    pages = {302-312},
    abstract = {Cell phenotype transition refers to the changes in the morphology, function, and surface markers of cells that occur under specific environmental conditions or physiological states, based on their genomic information and external signals. This process plays an important role in development, tissue repair, and responses to external stimuli such as infection or inflammation. Traditional bioinformatics methods for addressing cell type transition often rely on hypothesis-driven models, which may not fully capture the complexity and heterogeneity of the transition processes. In this paper, we introduce DualCPT, a cell phenotype transition and differentiation model based on Markov processes. Specifically, the model consists of a classification branch and a transition branch. The transition branch identifies regulatory genes involved in cell phenotype transition and differentiation. In the classification branch, we evaluate the model’s overall performance on general cell type classification tasks using a comprehensive multi-metric evaluation framework; in the transition branch, we implement a token pruning-based approach for critical locus discovery and enhance information interaction between full-sequence contexts and prioritized regulatory sites via an improved multi-head attention mechanism. Cell phenotype transition tasks are further assessed by uncertainty quantification and confidence calibration. In particular, in gene knockout experiments, we found that knocking out important genes alters the probability of cell phenotype transition and differentiation, and knocking out a certain number of essential genes can terminate specific transition processes. Data, code, and checkpoints are publicly available at https://github.com/Ssupercoder/DualCPT.}
}

@InProceedings{yao26,
    title = {Domain-Specific Expert Pruning for Mixture-of-Experts LLMs},
    author = {Yao, Juntao and Zheng, Huiyuan and Wang, Boyang and Yu, Xiaohu and Li, Yibo and Cao, Shaosheng and Di, Donglin and Wang, Boyan and Zheng, Haoyun and Yu, Jinze and Le, Anjie and Guo, Hongcheng},
    pages = {313-321},
    abstract = {Mixture-of-Experts (MoE) architectures have emerged as a promising paradigm for scaling large language models (LLMs) with sparse activation of task-specific experts. Despite their computational efficiency during inference, the massive overall parameter footprint of MoE models (e.g., GPT-4) introduces critical challenges for practical deployment. Current pruning approaches often fail to address two inherent characteristics of MoE systems: 1).intra-layer expert homogeneity where experts within the same MoE layer exhibit functional redundancy, and 2). inter-layer similarity patterns where deeper layers tend to contain progressively more homogeneous experts. To tackle these issues, we propose Cluster-driven Domain-Specific Expert Pruning (C-PRUNE), a novel two-stage framework for adaptive task-specific compression of MoE LLMs. C-PRUNE operates through layer-wise expert clustering, which groups functionally similar experts within each MoE layer using parameter similarity metrics, followed by global cluster pruning, which eliminates redundant clusters across all layers through a unified importance scoring mechanism that accounts for cross-layer homogeneity. We validate C-PRUNE through extensive experiments on multiple MoE models and benchmarks. The results demonstrate that C-PRUNE effectively reduces model size while outperforming existing MoE pruning methods. The effectiveness is observed across diverse domains, with notable performance in the medical field. We provide code. https://github.com/Fighoture/ MoE_unsupervised_pruning}
}

@InProceedings{zafari26,
    title = {The Need to Move Beyond Explainability Toward Chain-of-Thought Reasoning: A Focus on AI for Mammography},
    author = {Zafari, Yalda and Soliman, Shahd and Rahed, Essam A. and Mabrok, Mohamed},
    pages = {322-329},
    abstract = {Mammography is one of the primary imaging modalities for breast cancer screening and diagnosis, playing a pivotal role in early detection and mortality reduction. To alleviate the burden on radiologists interpreting mammographic data, artificial intelligence-based models have emerged as decision-making assistants, demonstrating promising results in several studies. However, a critical gap remains between AI capabilities and clinical integration. Current AI systems predominantly employ end-to-end classification approaches that bypass the structured, multi-step reasoning radiologists use in practice. Radiologists systematically detect abnormalities, characterize their features using standardized descriptors, correlate findings across imaging views, perform temporal comparisons, assess information sufficiency, and synthesize evidence into risk-stratified recommendations. In contrast, most AI models map directly from images to diagnoses without transparent intermediate reasoning, limiting their interpretability, clinical utility, and ability to generalize beyond training distributions. This paper examines the radiological chain-of-thought process in mammography interpretation, reviews current AI approaches and their limitations, and proposes a multi-stage reasoning framework that explicitly models each step of clinical decision-making. By decomposing the diagnostic task into sequential reasoning-based stages, this framework aims to create AI systems that not only predict outcomes but also reason transparently in alignment with clinical workflow. Crucially, we explain how contextual information such as breast density serves as a conditioning variable that dynamically optimizes subsequent reasoning stages, mirroring the adaptive decision-making process radiologists employ in clinical practice. We discuss the implications of this approach and identify critical dataset limitations that must be addressed to enable the development of truly reasoning-aware AI in breast imaging.}
}